kable_styling()
anion_list <- table(data$X) %>% sort(decreasing = TRUE) %>% names()
subset <- data %>% filter(X == anion_list[k])
table(subset$GroupCat) %>%
sort(decreasing = TRUE) %>%
t() %>%
kable("latex", longtable = T, booktabs = T,
caption = paste("Sample sizes of target categories in Anion", anion_list[k])) %>%
kable_styling()
library(tidyverse)
library(magrittr)
library(ggsci)
library(kableExtra)
library(glmnet)
library(caret)
library(gbm)
library(plotly)
source("scripts/functions.R")
data <-
read.csv("data/filledDatabaseNUMONLY_051820.csv") %>%
clean_data() %>%
filter(GroupCat %in% c(2,3,4,5,6)) %>%
mutate(GroupCat ,
GroupCat = recode(GroupCat,
"2" = "LiNb03",
"4" = "NCOT",
"3" = "Cubic",
"5" = "Tilted",
"6" = "Hexagonal"),
GroupCat = factor(GroupCat,
levels = c("Cubic","Tilted","Hexagonal","LiNb03","NCOT")))
table(data$X) %>% sort(decreasing = TRUE)
subset <- data %>% filter(X == "O")
table(subset$GroupCat) %>% sort(decreasing = TRUE)
X <- subset[,-c(1:4)] %>% remove_identical_cal() %>% as.matrix()
Y <- subset$GroupCat %>% droplevels() %>% as.matrix()
pca <- prcomp(X, scale = TRUE)
summary(pca)
X_PC <- pca$x[,1:17] %>% as.matrix()
PC_point <- data.frame(Compound = subset$Compound,
Cluster = as.character(subset$GroupCat),
X_PC)
PC <- pca$rotation %>%
as.data.frame() %>%
select(PC1,PC2,PC3) %>%
rownames_to_column("variable") %>%
mutate(tag = "end",
contribution = PC1^2 + PC2^2 + PC3^2)
PC[,2:4] <- PC[,2:4]*30
PC_initial <- PC %>% mutate(tag = "start")
PC_initial[,2:4] = 0
PC_arrow <- bind_rows(PC, PC_initial)
PC_arrow %>%
group_by(variable) %>%
plot_ly() %>%
add_trace(
x = ~PC1,
y = ~PC2,
z = ~PC3,
color = ~contribution,
text = ~variable,
type = 'scatter3d', mode = 'lines', opacity = 1,
line = list(width = 6, reverscale = FALSE)
) %>%
add_trace(
data = PC_point,
x = ~PC1,
y = ~PC2,
z = ~PC3,
color = ~Cluster,
colors = "Dark2",
text = ~Compound,
type = 'scatter3d', mode = 'markers',
opacity = 0.9)
#### -------------   step 1: ridge   ------------- ####
subset2 <- data %>% filter(X == "O") %>% filter(GroupCat != "NCOT") %>% droplevels()
X <- subset2[,-c(1:4)] %>% remove_identical_cal() %>% as.matrix()
Y <- subset2$GroupCat %>% droplevels() %>% as.matrix()
set.seed(2020)
folds <- caret::createFolds(1:nrow(X), k = 5, list = TRUE, returnTrain = FALSE)
ridge_cv = cv.glmnet(x = X, y = Y, alpha = 0, nfolds = 5,
type.measure = "deviance", family = "multinomial")
tb_ridge = prediction_table(alpha = 0, lambda = ridge_cv$lambda.min)
#### -------------   step 2: lasso   ------------- ####
lasso_cv = cv.glmnet(x = X, y = Y, alpha = 1, nfolds = 5,
type.measure = "deviance", family = "multinomial")
tb_lasso = prediction_table(alpha = 0, lambda = lasso_cv$lambda.min)
tb_lasso$r %>% print_accurate_tb()
tb_lasso$t[,-5] %>% highlight_tb_count()
tb_lasso$t[,-5] %>% highlight_tb_percent()
alpha = 0
#### -------------   step 2: lasso   ------------- ####
lasso_cv = cv.glmnet(x = X, y = Y, alpha = 1, nfolds = 5,
type.measure = "deviance", family = "multinomial")
tb_lasso = prediction_table(alpha = 1, lambda = lasso_cv$lambda.min)
tb_lasso$r %>% print_accurate_tb()
tb_lasso$t[,-5] %>% highlight_tb_count()
#### -------------   step 2: lasso   ------------- ####
lasso_cv = cv.glmnet(x = X, y = Y, alpha = 1, nfolds = 5,
type.measure = "deviance", family = "multinomial")
tb_lasso = prediction_table(alpha = 1, lambda = lasso_cv$lambda.min)
tb_lasso$r %>% print_accurate_tb()
#### -------------   step 1: ridge   ------------- ####
subset2 <- data %>% filter(X == "O") %>% filter(GroupCat != "NCOT") %>% droplevels()
X <- subset2[,-c(1:4)] %>% remove_identical_cal() %>% as.matrix()
Y <- subset2$GroupCat %>% droplevels() %>% as.matrix()
set.seed(2020)
#### -------------   step 1: ridge   ------------- ####
subset2 <- data %>% filter(X == "O") %>% filter(GroupCat != "NCOT") %>% droplevels()
X <- subset2[,-c(1:4)] %>% remove_identical_cal() %>% as.matrix()
Y <- subset2$GroupCat %>% droplevels() %>% as.matrix()
folds <- caret::createFolds(1:nrow(X), k = 5, list = TRUE, returnTrain = FALSE)
ridge_cv = cv.glmnet(x = X, y = Y, alpha = 0, nfolds = 5,
type.measure = "deviance", family = "multinomial")
tb_ridge = prediction_table(alpha = 0, lambda = ridge_cv$lambda.min)
tb_ridge$r %>% print_accurate_tb()
#### -------------   step 2: lasso   ------------- ####
lasso_cv = cv.glmnet(x = X, y = Y, alpha = 1, nfolds = 5,
type.measure = "deviance", family = "multinomial")
tb_lasso = prediction_table(alpha = 1, lambda = lasso_cv$lambda.min)
tb_lasso$r %>% print_accurate_tb()
folds <- caret::createFolds(1:nrow(X), k = 5, list = TRUE, returnTrain = FALSE)
ridge_cv = cv.glmnet(x = X, y = Y, alpha = 0, nfolds = 5,
type.measure = "deviance", family = "multinomial")
tb_ridge = prediction_table(alpha = 0, lambda = ridge_cv$lambda.min)
tb_ridge$r %>% print_accurate_tb()
#### -------------   step 2: lasso   ------------- ####
lasso_cv = cv.glmnet(x = X, y = Y, alpha = 1, nfolds = 5,
type.measure = "deviance", family = "multinomial")
tb_lasso = prediction_table(alpha = 1, lambda = lasso_cv$lambda.min)
tb_lasso$r %>% print_accurate_tb()
#### -------------   step 3: elastic net   ------------- ####
elastic_cv <-
train(GroupCat ~., data = data.frame(X, GroupCat = Y), method = "glmnet",
trControl = trainControl("cv", number = 5),
tuneLength = 10)
tb_elastic = prediction_table(alpha = elastic_cv$bestTune[[1]],
lambda = elastic_cv$bestTune[[2]])
tb_elastic$r %>% print_accurate_tb()
#### -------------   section 2: GBM   ------------- ####
gbm_cv <- gbm(GroupCat~., data = subset2[,-c(1:3)],
shrinkage = 0.01, distribution = "multinomial",
cv.folds = 5, n.trees = 3000, verbose = F)
best.iter = gbm.perf(gbm_cv, method="cv")
summary(gbm_cv) %>% View()
variable_importance <- summary(gbm_cv)
fitControl = trainControl(method = "cv", number = 5, returnResamp = "all")
model2 = train(GroupCat~., data = subset2[,-c(1:3)], method = "gbm",
distribution = "multinomial", trControl = fitControl, verbose=F,
tuneGrid = data.frame(.n.trees = best.iter,
.shrinkage = 0.01,
.interaction.depth = 1,
.n.minobsinnode = 1))
model2
tb = confusionMatrix(model2)$table %>% as.matrix()
tb_sum = colSums(tb)
tb / tb_sum
tb_importance <- summary(gbm_cv)
summary(gbm_cv) %>% unclass()
tb_importance <- summary(gbm_cv)$rel.inf
tb_importance <- summary(gbm_cv)$rel.inf
tb_importance <- summary(gbm_cv)$var
tb_importance <- summary(gbm_cv)$var
head(tb_importance)
#### -------------   step 5: which we predict it wrong   ------------- ####
top3 <- tb_importance[1:3]
X <- subset[,-c(1:4)] %>% remove_identical_cal() %>% as.matrix()
X_top3 <- X[,top3]
data.frame(
Compound = subset$Compound,
Cluster = as.character(subset$GroupCat),
X_top3
) %>%
plot_ly() %>%
add_trace(
x = ~ToleranceBVP,
y = ~IonizationPotentialofA,
z = ~CrystalRadiusofA,
color = ~Cluster,
colors = "Paired",
text = ~Compound,
type = 'scatter3d', mode = 'markers',
opacity = 0.8
)
top3
#### -------------   step 5: which we predict it wrong   ------------- ####
top3 <- tb_importance[1:3] %>% droplevels()
X <- subset[,-c(1:4)] %>% remove_identical_cal() %>% as.matrix()
X_top3 <- X[,top3]
data.frame(
Compound = subset$Compound,
Cluster = as.character(subset$GroupCat),
X_top3
) %>%
plot_ly() %>%
add_trace(
x = ~ToleranceBVP,
y = ~IonizationPotentialofA,
z = ~CrystalRadiusofA,
color = ~Cluster,
colors = "Paired",
text = ~Compound,
type = 'scatter3d', mode = 'markers',
opacity = 0.8
)
top3
X_top3
tb_importance[1:3]
X_top3 <- X %>% select(top3)
#### -------------   step 5: which we predict it wrong   ------------- ####
top3 <- c("ToleranceBVP","IonizationPotentialofA","CrystalRadiusofA")
X <- subset[,-c(1:4)] %>% remove_identical_cal() %>% as.matrix()
X_top3 <- X[,top3]
data.frame(
Compound = subset$Compound,
Cluster = as.character(subset$GroupCat),
X_top3
) %>%
plot_ly() %>%
add_trace(
x = ~ToleranceBVP,
y = ~IonizationPotentialofA,
z = ~CrystalRadiusofA,
color = ~Cluster,
colors = "Paired",
text = ~Compound,
type = 'scatter3d', mode = 'markers',
opacity = 0.8
)
#### -------------   section 1: multinomial reg   ------------- ####
subset2 <- data %>% filter(X == "O") %>% filter(GroupCat != "NCOT") %>% droplevels()
X <- subset2[,-c(1:4)] %>% remove_identical_cal() %>% as.matrix()
Y <- subset2$GroupCat %>% droplevels() %>% as.matrix()
set.seed(2020)
folds <- caret::createFolds(1:nrow(X), k = 5, list = TRUE, returnTrain = FALSE)
#### -------------   section 1: multinomial reg   ------------- ####
subset2 <- data %>% filter(X == "O") %>% filter(GroupCat != "NCOT") %>% droplevels()
X <- subset2[,-c(1:4)] %>% remove_identical_cal() %>% as.matrix()
Y <- subset2$GroupCat %>% droplevels() %>% as.matrix()
folds <- caret::createFolds(1:nrow(X), k = 5, list = TRUE, returnTrain = FALSE)
#### ridge
ridge_cv = cv.glmnet(x = X, y = Y, alpha = 0, nfolds = 5, type.measure = "deviance", family = "multinomial")
tb_ridge = prediction_table(alpha = 0, lambda = ridge_cv$lambda.min)
tb_ridge$r %>% print_accurate_tb()
#### ridge
ridge_cv = cv.glmnet(x = X, y = Y, alpha = 0, nfolds = 5, type.measure = "deviance", family = "multinomial")
tb_ridge = prediction_table(alpha = 0, lambda = ridge_cv$lambda.min)
tb_ridge$r %>% print_accurate_tb()
#### ridge
ridge_cv = cv.glmnet(x = X, y = Y, alpha = 0, nfolds = 5, type.measure = "deviance", family = "multinomial")
tb_ridge = prediction_table(alpha = 0, lambda = ridge_cv$lambda.min)
tb_ridge$r %>% print_accurate_tb()
#### ridge
ridge_cv = cv.glmnet(x = X, y = Y, alpha = 0, nfolds = 5, type.measure = "deviance", family = "multinomial")
tb_ridge = prediction_table(alpha = 0, lambda = ridge_cv$lambda.min)
tb_ridge$r %>% print_accurate_tb()
folds <- caret::createFolds(1:nrow(X), k = 5, list = TRUE, returnTrain = FALSE)
#### ridge
ridge_cv = cv.glmnet(x = X, y = Y, alpha = 0, nfolds = 5, type.measure = "deviance", family = "multinomial")
tb_ridge = prediction_table(alpha = 0, lambda = ridge_cv$lambda.min)
tb_ridge$r %>% print_accurate_tb()
folds <- caret::createFolds(1:nrow(X), k = 5, list = TRUE, returnTrain = FALSE)
#### ridge
ridge_cv = cv.glmnet(x = X, y = Y, alpha = 0, nfolds = 5, type.measure = "deviance", family = "multinomial")
tb_ridge = prediction_table(alpha = 0, lambda = ridge_cv$lambda.min)
tb_ridge$r %>% print_accurate_tb()
folds <- caret::createFolds(1:nrow(X), k = 5, list = TRUE, returnTrain = FALSE)
#### ridge
ridge_cv = cv.glmnet(x = X, y = Y, alpha = 0, nfolds = 5, type.measure = "deviance", family = "multinomial")
tb_ridge = prediction_table(alpha = 0, lambda = ridge_cv$lambda.min)
tb_ridge$r %>% print_accurate_tb()
set.seed(2020)
folds <- caret::createFolds(1:nrow(X), k = 5, list = TRUE, returnTrain = FALSE)
#### ridge
ridge_cv = cv.glmnet(x = X, y = Y, alpha = 0, nfolds = 5, type.measure = "deviance", family = "multinomial")
tb_ridge = prediction_table(alpha = 0, lambda = ridge_cv$lambda.min)
tb_ridge$r %>% print_accurate_tb()
#### elastic net
elastic_cv <-
train(GroupCat ~., data = data.frame(X, GroupCat = Y), method = "glmnet",
trControl = trainControl("cv", number = 5), tuneLength = 10)
tb_elastic = prediction_table(alpha = elastic_cv$bestTune[[1]], lambda = elastic_cv$bestTune[[2]])
tb_elastic$r %>% print_accurate_tb()
lasso_cv %>% get_coef()
#### lasso
lasso_cv = cv.glmnet(x = X, y = Y, alpha = 1, nfolds = 5, type.measure = "deviance", family = "multinomial")
lasso_cv %>% get_coef()
lasso_cv %>% get_coef(tuning_parameter = lasso_cv$lambda.min)
#### CI for lasso
B <- 1000
dummy_coef <- list()
#### CI for lasso
B <- 10
dummy_coef <- list()
#### CI for lasso
B <- 10
dummy_coef <- list()
for (i in 1:B){
dummy_coef[i] <- lasso_cv %>% get_coef(tuning_parameter = lasso_cv$lambda.min)
}
head(dummy_coef)
length(dummy_coef)
dummy_coef[1]
dummy_coef[[1]]
lasso_cv %>% get_coef(tuning_parameter = lasso_cv$lambda.min)
lasso_cv %>% get_coef(tuning_parameter = lasso_cv$lambda.min) %>% class()
#### CI for lasso
B <- 10
dummy_coef <- list()
for (i in 1:B){
dummy_coef[[i]] <- lasso_cv %>% get_coef(tuning_parameter = lasso_cv$lambda.min) %>% class()
}
dummy_coef[[1]]
#### CI for lasso
B <- 10
dummy_coef <- list()
for (i in 1:B){
dummy_coef[[i]] <- lasso_cv %>% get_coef(tuning_parameter = lasso_cv$lambda.min)
}
#### -------------   section 2: GBM   ------------- ####
gbm_cv <- gbm(GroupCat~., data = subset2[,-c(1:3)],
shrinkage = 0.01, distribution = "multinomial",
cv.folds = 5, n.trees = 3000, verbose = F)
#### CI for lasso
B <- 10
dummy_coef <- list()
for (i in 1:B){
dummy_coef[[i]] <- lasso_cv %>% get_coef(tuning_parameter = lasso_cv$lambda.min)
}
dummy_coef
dummy_coef[[1]]
rows_to_take <- sample(nrow(X), nrow(X))
#### CI for lasso
B <- 10
dummy_coef <- list()
for (i in 1:B){
rows_to_take <- sample(nrow(X), nrow(X))
lasso_cv <- cv.glmnet(x = X[rows_to_take,], y = Y[rows_to_take], alpha = 1, nfolds = 5, type.measure = "deviance", family = "multinomial")
dummy_coef[[i]] <- lasso_cv %>% get_coef(tuning_parameter = lasso_cv$lambda.min)
}
#### CI for lasso
B <- 10
cubic = hexagonal = linb = tilted = data.frame()
rows_to_take <- sample(nrow(X), nrow(X))
lasso_cv <- cv.glmnet(x = X[rows_to_take,], y = Y[rows_to_take], alpha = 1, nfolds = 5, type.measure = "deviance", family = "multinomial")
lasso_cv %>% get_coef(tuning_parameter = lasso_cv$lambda.min)
cubic[,i] <- tb_coef[,1]
tb_coef <- lasso_cv %>% get_coef(tuning_parameter = lasso_cv$lambda.min)
cubic[,i] <- tb_coef[,1]
hexagonal[,i] <-  tb_coef[,2]
cubic[,i] <- tb_coef[,1]
tb_coef[,1]
cubic[,i] <- tb_coef[,2]
cubic = hexagonal = linb = tilted = data.frame()
View(cubic)
cubic = hexagonal = linb = tilted = data.frame(NULL)
cubic[,i] <- tb_coef[,2]
cubic = hexagonal = linb = tilted = c()
cubic[,i] <- tb_coef[,2]
cubic = hexagonal = linb = tilted = X[FALSE,]
cubic[,i] <- tb_coef[,2]
cubic[,i] <- tb_coef[,2]
cubic[,3] <- tb_coef[,2]
#### CI for lasso
B <- 10
cubic = hexagonal = linb = tilted = X[FALSE,]
for (i in 1:B){
rows_to_take <- sample(nrow(X), nrow(X))
lasso_cv <- cv.glmnet(x = X[rows_to_take,], y = Y[rows_to_take], alpha = 1, nfolds = 5, type.measure = "deviance", family = "multinomial")
tb_coef <- lasso_cv %>% get_coef(tuning_parameter = lasso_cv$lambda.min)
cubic[,i] <- tb_coef[,2]
hexagonal[,i] <-  tb_coef[,3]
linb[,i] <-  tb_coef[,4]
titled[,i] <-  tb_coef[,5]
}
#### CI for lasso
B <- 10
cubic = hexagonal = linb = tilted = X[FALSE,]
for (i in 1:B){
rows_to_take <- sample(nrow(X), nrow(X))
lasso_cv <- cv.glmnet(x = X[rows_to_take,], y = Y[rows_to_take], alpha = 1, nfolds = 5, type.measure = "deviance", family = "multinomial")
tb_coef <- lasso_cv %>% get_coef(tuning_parameter = lasso_cv$lambda.min)
cubic[,i] <- tb_coef[,2]
hexagonal[,i] <-  tb_coef[,3]
linb[,i] <-  tb_coef[,4]
tilted[,i] <-  tb_coef[,5]
}
View(cubic)
rows_to_take <- sample(nrow(X), nrow(X))
lasso_cv <- cv.glmnet(x = X[rows_to_take,], y = Y[rows_to_take], alpha = 1, nfolds = 5, type.measure = "deviance", family = "multinomial")
tb_coef <- lasso_cv %>% get_coef(tuning_parameter = lasso_cv$lambda.min)
tb_coef[,2]
cubic = hexagonal = linb = tilted = X[1,]
View(cubic)
cubic = hexagonal = linb = tilted = t(X[1,])
cubic[i,] <- t(tb_coef[,2])
i = 1
cubic[i,] <- t(tb_coef[,2])
tb_coef[,2]
t(tb_coef[,2])
cubic[i,] <- t(tb_coef[,2])
cubic[i,] <- tb_coef[,2]
tb_coef[,2]
tb_coef
cubic[i,] <- tb_coef[-1,2]
View(tb_coef)
tb_coef[,2]
cubic = hexagonal = linb = tilted = data.frame(t(X[1,]))
cubic = hexagonal = linb = tilted = data.frame(Intercept = 0, t(X[1,]))
#### CI for lasso
B <- 10
cubic = hexagonal = linb = tilted = data.frame(Intercept = 0, t(X[1,]))
cubic[i,] <- tb_coef[,2]
hexagonal[,i] <-  tb_coef[,3]
cubic = hexagonal = linb = tilted = data.frame(Intercept = 0, t(X[1,]))
cubic[i,] <- tb_coef[,2]
hexagonal[i,] <-  tb_coef[,3]
linb[i,] <-  tb_coef[,4]
tilted[i,] <-  tb_coef[,5]
#### CI for lasso
B <- 2000
cubic = hexagonal = linb = tilted = data.frame(Intercept = 0, t(X[1,]))
for (i in 1:B){
rows_to_take <- sample(nrow(X), nrow(X))
lasso_cv <- cv.glmnet(x = X[rows_to_take,], y = Y[rows_to_take], alpha = 1, nfolds = 5, type.measure = "deviance", family = "multinomial")
tb_coef <- lasso_cv %>% get_coef(tuning_parameter = lasso_cv$lambda.min)
cubic[i,] <- tb_coef[,2]
hexagonal[i,] <-  tb_coef[,3]
linb[i,] <-  tb_coef[,4]
tilted[i,] <-  tb_coef[,5]
}
View(cubic)
apply(cubic, 2, function(col) quantile(col, probs = c(0.025,0.975)))
rows_to_take <- sample(nrow(X), nrow(X))
ridge_cv = cv.glmnet(x = X, y = Y, alpha = 0, nfolds = 5, type.measure = "deviance", family = "multinomial")
tb_coef <- ridge_cv %>% get_coef(tuning_parameter = ridge_cv$lambda.min)
#### CI for multinomial reg
B <- 500
cubic = hexagonal = linb = tilted = data.frame(Intercept = 0, t(X[1,]))
#### CI for multinomial reg
B <- 5
cubic = hexagonal = linb = tilted = data.frame(Intercept = 0, t(X[1,]))
for (i in 1:B){
rows_to_take <- sample(nrow(X), nrow(X))
ridge_cv = cv.glmnet(x = X, y = Y, alpha = 0, nfolds = 5, type.measure = "deviance", family = "multinomial")
tb_coef <- ridge_cv %>% get_coef(tuning_parameter = ridge_cv$lambda.min)
# lasso_cv <- cv.glmnet(x = X[rows_to_take,], y = Y[rows_to_take], alpha = 1, nfolds = 5, type.measure = "deviance", family = "multinomial")
# tb_coef <- lasso_cv %>% get_coef(tuning_parameter = lasso_cv$lambda.min)
cubic[i,] <- tb_coef[,2]
hexagonal[i,] <-  tb_coef[,3]
linb[i,] <-  tb_coef[,4]
tilted[i,] <-  tb_coef[,5]
}
apply(cubic, 2, function(col) quantile(col, probs = c(0.025,0.975)))
apply(hexagonal, 2, function(col) quantile(col, probs = c(0.025,0.975)))
apply(linb, 2, function(col) quantile(col, probs = c(0.025,0.975)))
apply(tilted, 2, function(col) quantile(col, probs = c(0.025,0.975)))
B <- 100
cubic = hexagonal = linb = tilted = data.frame(Intercept = 0, t(X[1,]))
for (i in 1:B){
rows_to_take <- sample(nrow(X), nrow(X))
ridge_cv = cv.glmnet(x = X, y = Y, alpha = 0, nfolds = 5, type.measure = "deviance", family = "multinomial")
tb_coef <- ridge_cv %>% get_coef(tuning_parameter = ridge_cv$lambda.min)
# lasso_cv <- cv.glmnet(x = X[rows_to_take,], y = Y[rows_to_take], alpha = 1, nfolds = 5, type.measure = "deviance", family = "multinomial")
# tb_coef <- lasso_cv %>% get_coef(tuning_parameter = lasso_cv$lambda.min)
cubic[i,] <- tb_coef[,2]
hexagonal[i,] <-  tb_coef[,3]
linb[i,] <-  tb_coef[,4]
tilted[i,] <-  tb_coef[,5]
}
apply(cubic, 2, function(col) quantile(col, probs = c(0.025,0.975)))
apply(hexagonal, 2, function(col) quantile(col, probs = c(0.025,0.975)))
apply(linb, 2, function(col) quantile(col, probs = c(0.025,0.975)))
apply(tilted, 2, function(col) quantile(col, probs = c(0.025,0.975)))
B <- 2000
cubic = hexagonal = linb = tilted = data.frame(Intercept = 0, t(X[1,]))
for (i in 1:B){
rows_to_take <- sample(nrow(X), nrow(X))
ridge_cv = cv.glmnet(x = X, y = Y, alpha = 0, nfolds = 5, type.measure = "deviance", family = "multinomial")
tb_coef <- ridge_cv %>% get_coef(tuning_parameter = ridge_cv$lambda.min)
# lasso_cv <- cv.glmnet(x = X[rows_to_take,], y = Y[rows_to_take], alpha = 1, nfolds = 5, type.measure = "deviance", family = "multinomial")
# tb_coef <- lasso_cv %>% get_coef(tuning_parameter = lasso_cv$lambda.min)
cubic[i,] <- tb_coef[,2]
hexagonal[i,] <-  tb_coef[,3]
linb[i,] <-  tb_coef[,4]
tilted[i,] <-  tb_coef[,5]
}
apply(cubic, 2, function(col) quantile(col, probs = c(0.025,0.975)))
apply(hexagonal, 2, function(col) quantile(col, probs = c(0.025,0.975)))
apply(linb, 2, function(col) quantile(col, probs = c(0.025,0.975)))
apply(tilted, 2, function(col) quantile(col, probs = c(0.025,0.975)))
apply(cubic, 2, function(col) quantile(col, probs = c(0.025,0.975)))
apply(cubic, 2, function(col) quantile(col, probs = c(0.025,0.975))) %>% dim()
t1 = apply(cubic, 2, function(col) quantile(col, probs = c(0.025,0.975)))
t2 = apply(hexagonal, 2, function(col) quantile(col, probs = c(0.025,0.975)))
t3 = apply(linb, 2, function(col) quantile(col, probs = c(0.025,0.975)))
t4 = apply(tilted, 2, function(col) quantile(col, probs = c(0.025,0.975)))
bind_rows(
t1,t2,t3,t4
) %>%
write.csv(file = "markdown/coefCI.csv")
bind_rows(
t1,t2,t3,t4
)
t1
rbind(
t1,t2,t3,t4
)
rbind(
t1,t2,t3,t4
) %>%
write.csv(file = "markdown/coefCI.csv")
